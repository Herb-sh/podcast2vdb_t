{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<module 'db' from '/Users/herbishtini/Documents/UNI/Projektarbeit/podcast2vdb/src/db.py'>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eigene Funtionen aus dem src-Ordner des übergeordneten Verzeichnis importieren\n",
    "import random\n",
    "import sys, os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "LIBPATH = os.path.abspath('../src')\n",
    "if not LIBPATH in sys.path: sys.path.insert(1, LIBPATH)\n",
    "import feed as feed\n",
    "import core as core\n",
    "import db as db\n",
    "#\n",
    "import importlib\n",
    "#\n",
    "importlib.reload(core)\n",
    "importlib.reload(feed)\n",
    "importlib.reload(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "podcasts = feed.search_podcast('Knowledge Science - Alles über KI, ML und NLP')\n",
    "#podcasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1383759"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "podcast_id = podcasts['feeds'][0]['id']\n",
    "podcast_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.buzzsprout.com/1687822/13887470-episode-100-ruckblick-ausblick-wo-geht-die-reise-von-llms-hin.mp3\n",
      "https://www.buzzsprout.com/1687822/13820272-episode-99-marc-fischer-luca-beurer-kellner-lmql-ai.mp3\n",
      "https://www.buzzsprout.com/1687822/13776306-episode-98-dspy-llms-als-devices.mp3\n",
      "https://www.buzzsprout.com/1687822/13733008-episode-97-contrastive-decoding-fact-checking-in-sprachmodellen.mp3\n",
      "https://www.buzzsprout.com/1687822/13692579-episode-96-language-model-programming.mp3\n"
     ]
    }
   ],
   "source": [
    "episodes = feed.get_episodes(podcast_id)\n",
    "\n",
    "for episode in episodes[0:min(5, len(episodes))]:\n",
    "    print(episode['enclosureUrl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(16051464880,\n 'https://www.buzzsprout.com/1687822/13887470-episode-100-ruckblick-ausblick-wo-geht-die-reise-von-llms-hin.mp3')"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://podcastindex.org/podcast/2481152?episode=15580091589\n",
    "# https://radioplus-bloomberg.akamaized.net/syndicatedaudio/MarketMinutesWeekday.mp3\n",
    "#episode_id, episode_url = '2334', 'https://radioplus-bloomberg.akamaized.net/syndicatedaudio/MarketMinutesWeekday.mp3'\n",
    "episode_id, episode_url= episodes[0]['id'], episodes[0]['enclosureUrl']\n",
    "episode_id, episode_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.buzzsprout.com/1687822/13887470-episode-100-ruckblick-ausblick-wo-geht-die-reise-von-llms-hin.mp3 already downloaded!\n"
     ]
    }
   ],
   "source": [
    "filename_audio = feed.download_episode(episode_url, episode_id=episode_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing episode 16051464880 ...\n",
      "Running on cpu with compute type int8 ...\n",
      "Loading model tiny ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.1.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../../.cache/torch/whisperx-vad-segmentation.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.0.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.1.0. Bad things might happen unless you revert torch to 1.x.\n",
      "Loading audio file ../data/raw/16051464880.mp3 ...\n",
      "41082044\n",
      "Transcribing audio content ...\n",
      "Loading alignment model ... {'segments': [{'text': ' Hallo und herzlich Willkommen zu Hunderts ein Evisote. Heute sprechen wir über die Historie des Podcasts und über die Zukunft der Sprachmodelle. Bleiben Sie dran. Auch diese Sendung wird von Excel 2 gesponsert. Excel 2 ist ein schon Wenscher von Audium-Kippgemene.', 'start': 0.009, 'end': 14.241}, {'text': ' Knowledge Science. Der Podcast über künstliche Intelligenz im Allgemeinen und Natural Language Processing im Spezialen. Mittels KI-Bissen entdecken, aufbereiten und nutzbar machen. Das ist die Idee hinter Knowledge Science. Durch Entmüsteifizierung der künstlichen Intelligenz und vielen praktischen Interviews machen wir dieses Thema Wechendlich-Greifbar. Willkommen zum Podcasts von Siegurt Schacht und Karsten Lankion.', 'start': 20.572, 'end': 49.206}, {'text': ' Hallo Kasten. Hallo Siegurt. Schön, hast du heute wieder dabei bist? Ja, freut mich auch, insbesondere, was die Hunderte besod ist. Genau, das muss gefeiert werden. Ja, ein Gefeiert. Hunderte Episodeen. Wir doiden eigentlich, wenn wir uns genau rund 50 Stunden unser Gepapel. Ja, jetzt du gedacht, dass wir das so lange durchhalten.', 'start': 53.08, 'end': 72.568}, {'text': ' Wir hatten sehr gesagt in den ersten zwei Episodeen haben wir uns 5 Episodeen gegeben. Wenn man reinhört, ich glaube in der ersten und in der zweiten Episode haben wir über die Sprache. Dann gesagt, wir geben uns 5 oder 6 Episodeen. Und dann vermut man, dass es ähnlich schaffen. Oder das System ausgehen? Genau, das System ausgehen. Aber das ist ja nicht passiert in den letzten.', 'start': 72.619, 'end': 93.49}, {'text': ' 2,5 Jahre, wenn man es genau nimmt. Man muss natürlich dazu sagen, dass uns die Entwicklung der KI in den letzten zwei Jahren natürlich auch kräftig unterstützt hat. Ja, absolut. Also gerade das, was mit Touchivity und den Sprachmodellen, das hat er genau in die Kabel gestragen, wo wir uns bewegen wollten. Wir haben ja gesagt, der Podcast soll er genau in die Richtung gehen, KI aufzuklären und KI zu zeigen. Aber gleichzeitig auch Sprachmodelle und Sprach-Evolusion mit dem Fokus zu haben.', 'start': 93.49, 'end': 123.183}, {'text': ' Und das so viel passiert, dass man eigentlich immer unten Thema hatte, würde man eigentlich sagen. Absolut. Absolut. Vor allem immer mal sieht, wie sich der Podcast, wenn wir im Einspieler gesagt haben, wir werden im Mittel auch über die Historie des Podcast sprechen. Wenn man sieht, wie wir daran gegangen sind, jetzt können wir jetzt das Geheimnis lüften. Dann haben wir natürlich einen Masterplan gehabt und dann haben dann erst mal uns hingesetzt und uns für die nächsten drei Jahre überlegt, welche Themen werden kommen. Und dann natürlich die Episodeen runtergeplant und machen das Woche für Woche. Es ist demokratisch.', 'start': 124.514, 'end': 152.654}, {'text': ' Ich darf auch verraten, ich habe eigentlich nur mein Vorsitzungsgruppe genommen und habe ein einfacher Stunde für Stunde daraus geschnitten und das jetzt nicht. Weil das ist nicht wahr. Nein, es ist tatsächlich so, dass der Mendefit auch getreten von den neuen Themen sind und demensprechend eigentlich immer geguckt haben, was es gerade spannend in der Woche.', 'start': 153.319, 'end': 173.49}, {'text': ' Und dann in der Woche, sozusagen dieses Thema, was uns selber dann treibt, dann mit in den Podcast reingenommen haben. Und in der Anfangsphase, ja, ich hab mal versucht, die einzelnen Episoten, so ein bisschen zu strukturieren, ja, weil ich sagen, wie gesagt, runter Themen, einfach durchgehen, macht ja auch keinen Sinn. Und ganz spannend fand ich.', 'start': 173.49, 'end': 192.858}, {'text': ' dass wir im Endeffekt eigentlich gerade in der Anfangsfaser. Wir haben Februar angefangen in Februar 21, das war so der Stadtpunkt. Und dann haben wir angefangen von Februar bis April 21, 2021, vor allem über Grundlagen von KI zu sprechen, historischen Überblick zu geben, entmüsteffizieren. Also gerade dieses Thema KI eigentlich für jeder Mann jede Frau tatsächlich kräftere zu machen.', 'start': 192.858, 'end': 216.971}, {'text': ' Ja, deshalb sage ich ja, so ganz, es war zwar ein bisschen spaßig gesagt, da war ein Funkenbarheit, bei trotzdem dran. Das sind natürlich in Halte, die wir in den Vorlesen auch immer wieder bringen und so fangen, hatte man natürlich eine Basis auf die man aufsetzen konnte. Aber als vielleicht in einfachen Worten, wenn man zusammengefasst, was ist KI, versteckt der hinter. In einer insbesondere esmaschine Lern, als Basis, als Teilfunk KI, der vieles von dem, was wir jetzt haben, mich zusammen alle sind, der Form eigentlich erst ermöglicht.', 'start': 217.466, 'end': 243.763}, {'text': ' Ja und auch viele Grundlagen, weil man auch in HP Grundlagen war, die eine oder andere Episote fand ich auch ganz spannend, also in Silustank, die Sendungen von Februar bis April, wenn sie sich dann nochmal einfuchsten wollen, in das Thema KI an sich, hat dann für ich den Effekt, dass da noch nicht dieses ganze Thema generatifiker ISO stark betrieben war, weil das kann wir dann erst zumindest in der breiten Masse 22, 22, 22.', 'start': 244.36, 'end': 269.94}, {'text': ' Von daher ist es tatsächlich so die das klassische KI-Tema, eigentlich klassische Maschinenloining, Deep Learning.', 'start': 269.94, 'end': 277.108}, {'text': ' Genau, denn obwohl, ja, das Fundamentale Paper-Attention ist all unit von 2017, ja schon einige Jahre alt war, hat ja auch eine massive Durchbruch in die Aufmerksamkeit der Attention. Pass dann natürlich auch gleich wieder, kam ja erst etwa später mit dem Durchbruch von GPT 3, 4. GPT, das war so ein bisschen der Durchbruch, wo es flächendeckend, könnte man was sagen, eine Aufmerksamkeit erlangtärt in der Bevölkerung.', 'start': 277.637, 'end': 305.811}, {'text': ' und auch einen Zugriff. Also es war ja vor allem, wenn du überlegt, gebe die zwei und gebe die drei, wann er hinter der Geheimen oder in der gestricktierten Appie. Das heißt, du hast sich einen direkten Zugriff, nur Feinladung, bestimmte Institutionen und so weiter, sodass in der breiten Masse die Modelle ja gar nicht zu, wer arbeitet werden konnten. Und das hat sich ja dann im Endeffekt mit dem, gebe die drei, fünf oder so trätst du, gebe die geändert. Und dann ist es ja im Endeffekt, ja, wie jeder weiß, explodiert haben.', 'start': 306.391, 'end': 333.592}, {'text': ' Und das ist das ganz wichtig, das ist einfach zugriff. Das Frühjahr, ich meine, verschiedene Leunungen, die Blödinger, das insbesondere verschiedene Leunungen, gibt es schon lange. Die Anfänger haben wir auch da gestellt zurück, bis in die 40er-Jahr, ich hatte gerade den interessanten Briecht gelesen.', 'start': 333.592, 'end': 349.718}, {'text': ' wo als der Computer entwickelt wurde, dass sie die Grundstruktur mit, was kennen müssen, da von Neumann-Achtektur, dass zu der Zeit sogar Leute diskutiert hätten, macht man diese wirklich relativ stache, insozonsbasierte Achtektur, wie wir sie kennen über Jahrzehnte langen, oder sollte man da schon diese kreative Verarbeitung von Informationen, wie wir uns im Höhen mit Nordrhoden und wie nachbilden. Man hat sich immer für die von Neumann-Achtektur entschieden. Und so langsam gibt es da vielleicht Bewegungen, dass da zumindest als 2-Standbeinene andere Formen kommen.', 'start': 349.718, 'end': 379.292}, {'text': ' Aber im Zeit im 50ern haben wir das ja. Insofern war aber, dass das weißt, dass der Burglinisch ließen wollte, die die Verwendung war halt wenige Neuten vorbehalten, die halt auch mit überhaupt zugekumpft zu komputer hatten, die mit Programierungen auskennt, wenn das entsprechend nutzen konnten. Jetzt dieser einfach Zugriff, das wirklich', 'start': 379.292, 'end': 400.162}, {'text': ' Jeder, der uv. zu münternet hat, damit spielen kann. Es hat nicht selbst entwickeln vielleicht gleich, aber das nutzen kann, in Zukunft, auf diese Technologie hat. Das ist ein ganz, ganz anderer Art, wie wir jetzt ein Technologie bereitstellen, als das, was wir in den letzten Jahrzehnt mit anderen Technologien erlebt haben.', 'start': 400.162, 'end': 415.077}, {'text': ' Ja, und das führt ja dann auch zwangsläufig dazu, dass die Geschwindigkeit der Weiterentwicklung sich erkravierend ändert. Und das finde ich ganz spannend, das erheben wir uns aber glaube ich so ein paar Minuten auf, weil da will ich ganz gerne auch mal so ein bisschen groen, denn diese Sprachmodelle hin, also was ist die inwicklung der Sprachmodelle?', 'start': 416.323, 'end': 435.162}, {'text': ' Und da sieht man nämlich genau diesen Effekt, dass wenn du in die Breite Masse gehst, dass sich plötzlich die Fokus hier und der Art der Motene, wie sie gestruktur jetzt sind oder die Größe und so weiter, Masse verhindert und der Fokus auf eine ganz andere Richtung geht. Aber da würde ich dann gleich noch mal schauen. Ja genau. Genau, genau.', 'start': 435.162, 'end': 455.367}, {'text': ' Weil in das Hand war, von my bis Juni haben wir uns damit ein Fortschrittnis NLP und vor einem hardware Themen beschäftigt. Also ganz interessant. Ich habe ganz lustig, weil wir dann zwischen drin immer wieder gesprochen sind. Man hat natürlich auch immer das Thema, welche Ressusen bei euch dafür, wie trainiert man das und so weiter. Und dann ist es eigentlich so, dass wir eigentlich einen ganz interessanten ohne das wir es gemerkt haben, wahrscheinlich. August bis September 21.', 'start': 455.367, 'end': 483.439}, {'text': ' Da haben wir uns ganz stark auf Ethik und Geschäftsstrategik konzentriert. Wo kann man KI einsetzen, welche Auswirkungen hat das im Unternehmen, welche Ethischen Belangene sind, mit zu beachten und so weiter? Also, wir als ob man das so geplant hätte, von der Historie, über Vertiefung, Hinzu, Daten, Management und dann zur Ethik. Aber wie gesagt, ganz ehrlich, können wir ja auf die Sagen nach der Hundertsten Episode so richtig geplant haben wir eigentlich nichts? Nein, aber in Spannend ist ja, weil was er nachkommt.', 'start': 483.439, 'end': 513.422}, {'text': ' Dann kommen die mich ein großer Block mit dem Menschen in der Pipeline, immer schien Learning und Reinforcements Learning. Wichtig. Und was ja, ohne dass wir so derzeit natürlich wussten und natürlich auch nicht unsere Finger mit dem Spiel hatten, was aber natürlich ein wahnsinnig wichtiger Baustein ist für den Erfolg von神cipitiv. Ja, absolut das Reinforcements Learning mit Jungen und Feedback. Und das wird das im Endeffekt vorbereitet haben auf das Thema. Insohne Reinforcements Learning. Ist schon faszinierend.', 'start': 513.422, 'end': 542.807}, {'text': ' Ja, aber es zeigt ja, also letztendlich, meine Klappe, wir haben diese Technologie einfach aufgegriffen, weil sie existieren und weil sie eine gewisse Bedeutung haben. Aber es zeigt ja, dass alle diese Bausteine, die jetzt in solchen Technologien, wie wir Sprachmodellen und den erfolgreichen Einsatz zusammenkommen, ihr nicht in dem Moment alle nur entwickelt wurden, sondern teilweise schon seit Jahren oder Jahrzehnten existieren. Ja, absolut.', 'start': 542.995, 'end': 565.52}, {'text': ' Und das ist eigentlich schon in der gewissen Weise klar, ist, wie sich Themen miteinander kombinieren lassen aufdauer. Ob es dann funktioniert oder nicht, ist es dann anders. Dann gibt es mal so ein wie das in den Postman-Learning-Beschimm und Feedback, so ein zünden Effekt, wo man da daher klar super funktioniert.', 'start': 566.254, 'end': 582.892}, {'text': ' Und dann fällt mir auf, danach hatten wir einige Folgen, wo es um ein Thema gegen was uns ja im geradeem rechnolisch sein ist, aber sondern das wichtig ist, nämlich der Infomationsexakt von Ause Texten. Wie kriege ich Infomationse raus, wir wollten neunen Schrafen aufbauen, haben da Ansätze probiert, das mit Gapemisch speziellen Modellen, die das können, das würde man ja jetzt, wenn man knapp zwei Jahre später komplett anders macht.', 'start': 582.892, 'end': 605.674}, {'text': ' Ja, aber auch das, finde ich, wie da spannend. Weil im Endeffekt kommen wir auch wieder von der historische Podcast KI, dann hin zu den Sprachen, oder gelsen Sprachverarbeitung, Reinforcement Learning, dann Exaktion von Wissen, auch der Rheidung des Wissens verwendbar machen, als heute nichts anderes ist, als die so in einen Rackablikationen, die nicht ja versucht, Dokumente an so Sprachen, Modelle anzubinden, um dann mit den diskutieren zu können.', 'start': 606.101, 'end': 631.152}, {'text': ' Nur, dass man eigentlich ein bisschen Weckes von dem Reinen, ja, ist einmal Neulchkraft. Man sagt von wegen mit der Erspeicherung der Wissensfraktionen, den Form von Embeddings. Und dann, die diese manche Suche über diese Embeddings, haben wir eigentlich eine Effekt, der uns wahnsinnig gut hilft, diese Probleme, nämlich Fragen auf einem Wissensbasis lösen zu können, ja, und eine Stereue unterstützt zu werden.', 'start': 631.152, 'end': 658.797}, {'text': ' Interessant ist aber, dass ich jetzt so langsam, also zumindest das, was man in den Papers sieht und auch in den Diskussionen, dass ich langsam trotzdem so ein bisschen es wieder einschleicht, dass dann Wissenskaff gar nicht schlecht ist. So, dass man ein böttischen Kombinatorik hat aus,', 'start': 658.797, 'end': 674.48}, {'text': ' in einem reinen Bektordatenbank, wo im Endeffekt ein Textrumente in Form von Wektoren drin liegen, also in Form von Wektoren von Wektoren von Wektans. Und einen Wissenskrafen, der Mendeffekt strukturell auf Wektepforen herstellt, die zum Teil auch aus im Bettingsbestehene oder halte Mendeffektern ausfütlich dann den Informationen selber.', 'start': 674.48, 'end': 693.609}, {'text': ' Das glaube ich, wenn wir so in einem halben Jahr oder einem Jahr noch mal den Rückblick machen würden, dass wir wahrscheinlich ein paar Sendungen haben, wo wir sagen, ganz tolle Innovative ansetzen. Da ist wieder jemand, der in den Nodelschgrafen mit dem und dem verbindet, weil dann dadurch die Hallucination zurückgeht oder ähnliche Themen. Da bin ich mir sicher, dass das kommen. Es ist immer schon ein bisschen im Ausblick zukunft. Was kommt alles, aber ich bin völlig bei idea, dass wir bestimmte', 'start': 693.609, 'end': 718.148}, {'text': ' Die Ansätze haben unterschiedliche Stärken und Schwächen. Und wenn wir ein Wissensgrafen haben, können wir halt das wirklich explizit, dass die Wissensfragmente modulieren erfassen und machen sie Menschen direkt zugreifbar und auch weiterbar. Und wenn wir es schaffen, das natürlich zusammenzubringen.', 'start': 718.148, 'end': 735.964}, {'text': ' Können wir meine Meinung nach davon profitieren, eher als wenn wir ein Monster großes in Ordner als Netz haben, was alles umfasst und ständig aktualisiert werden muss. Ob sich das durchs jetzt oder nicht wird man sehen, aber das ist so vielleicht ein bisschen Thema auf jeden Ausblick. Ja, absolut.', 'start': 735.964, 'end': 751.442}, {'text': ' Und dann muss man sein, dann sind wir so ein bisschen in der Anfang 2022 angekommen. Und dann dankst du haben Drehs, sich das sehr stark auch bei uns in den Folgen, Richtung Sprachmodelle. Mit all seinen Fassetten, von denen ich sage mal Datenqualitäten, aktuellen Entwicklungen, KI-Trens, Innovationen, in Wuch haben wir uns anwenden, hin zu ganz kritischen Themen, sind wir bis November 2021 ungefähr.', 'start': 752.312, 'end': 776.561}, {'text': ' Und von November 22 an, bis October 23, haben wir uns vor allem halt, um die welche Modelle gibt es, welche Ausbildung gibt es, wie kann man die evaluieren, wohl die Modalität, waren großes Thema. Also man sieht dann langsam die Reiserichtung, den Schwerpunkt, den es auch in der Realen wird, momentan nimmt, das ist zumindest die Wahrnehmung, das ist halt vor allem die Sprachmodelle. Wir reden seit einem Jahr, aber fast nichts anderes als Sprachmodelle oder generative KI,', 'start': 776.561, 'end': 805.111}, {'text': ' Und wie man sie nutzt und was dafür wichtig ist? Richtig. Ja. Klasse da? Mhm. Und das sind das Handel finde ich, dass es immer noch nicht zu Ende ist. Also dass wir noch an einem über wie viel Themen ihr noch reden könnt. Und es auch tun werden, da bin ich mir sicher.', 'start': 805.111, 'end': 821.374}, {'text': ' Also wenn ich zum Beispiel momentan dann denke, ganz vielen Aspekten, wenn die Sprachmoder jetzt eingesetzt, man merkt jetzt, dass die Firmen anfangen zu sagen, ja, wir haben Oblication, wir wollen das umsetzen, als man es nicht mehr so in diesen reinen Bromten, also dem Motor, ich gibt mal was in Chedshibiti ein, sondern jetzt möchte man das ganz gerne auch in der Firmen integriert haben als Wissensdaten, wenn man als Assistent als Kommunikationspartner, als Rischarge, Werkzeug oder ähnliches.', 'start': 821.647, 'end': 846.852}, {'text': ' Und das natürlich über meine eigenen Dokumente und so weiter. Man merkt diese Retrieval-Opomended-Gineration, Obligationen, Rack-Applicationen, die halt diese kombinatorikose Wektordatenbank und Sprachmodell zu enttastellen, dass die jetzt halt immer mehr in dieser Welt kommen.', 'start': 846.852, 'end': 864.275}, {'text': ' Das ist dieser riesen Fokus auf die Sprachmodelle. Das zeigt natürlich, dass es dieser Parade gibt, wenn wechselt es in einer Halbde KI, dass man halt nicht nur gibt es auch noch. Das war mir nicht vernachlässig. Dass man nicht nur Zielgerichtet aufgabelt ist. Das sieht man da jetzt noch nie, was eine konkrete Aufgabe ist, sondern dass ich halt diese Sprachmodelle als generativen Ansatz habe und ich die Aufgabe jetzt nicht was im Prompt konkret mitgebe. Dann hoffe, dass ich eine Geschichte anvertrauskriege.', 'start': 864.275, 'end': 891.988}, {'text': ' dass dieser Schwing mit dem ich so viel machen kann. Es ist so wichtig, dass es erklärt, warum wir diesen Fokus stark darauf gesetzt haben und auch Parleboobachtbar von dem größer Fokus Prompting. Ich schreibe ich ein Promptik, was da aus. Aber es ist mal begeistert, dass es überhaupt geht. Jetzt hin, da hat der wirklich Monat lange eigentlich das dominiert und immer noch.', 'start': 891.988, 'end': 914.121}, {'text': ' Gibt ja wahrscheinlich sich Folgen, die man irgendwo was sich im Off Future brunterladen kann oder oder Großbuchen, wie kann ich mit ein richtigem Promt ganz wie Geld verdienen? Alles schöne gut. Aber Richtung, dass man gesagt, Engineering ist so nünftig angehen, wie die Systemate schrannen, bis hin, wie wird es möglicherweise so weit in der Grund gedrigen, dass ich am Ende wieder mit Sprachmodellen programmiere.', 'start': 914.121, 'end': 936.323}, {'text': ' Das ist, dass ich da, wie er hat, mehr, die das LML werden, das DSBY, dass ich halt wirklich das integrieren, dann sich das Language Model programming. Und dann kommen meine mich jetzt genau zu der Ebene, dass wir eigentlich am Ende Sprachmodelle, so ein bisschen als Processing als Computing Engine im Hintergrund haben. Und das ist wirklich eine Weise der Gegenentwurf, zu einem er an der klassischen von Neumannarchtik, für wo ich wirklich Instruktionen exaktbar arbeute.', 'start': 936.323, 'end': 962.159}, {'text': ' Wir sagen, ja, beide Modell sind wichtig. Ich brauche das eine, um exaktzeitgeregnis zu haben, wo ich wirklich genau weiß, was ich als Algorithmus geschrieben habe und was ich auskrieg. Und ich habe als Gegenstück, so ein bisschen dieses kreative Creative Computing. Und ich glaube, dass beide Elemente wichtig sind und die Frage ist, wie wir das halt im Bedarsfall zusammenbringen.', 'start': 962.159, 'end': 981.988}, {'text': ' Ja, es faszinierend. Da bin ich voll bei dir. Vor allem, wenn man überlegt, es gibt ja jetzt gerade in den letzten zwei Wochen sind wir da ein paar Papers rausgekommen. Eins, ich glaube es war von den Nvidia. Ich habe es jetzt gar nicht genau im Kopf, aber es ging darum, dass man im Endeffekt eine Roboter handtriniert hat, die einen Stift böttisch um die Finger eflipen soll, also so drum rotieren soll. Und da hat man Reinforcement Learning dafür verwendet. Und es spannende Wahrheit, dass man für das Reinforcement Learning braucht', 'start': 982.432, 'end': 1011.032}, {'text': ' man ja wie in den folgen schon erwähnt, braucht man ja im Endeffekt Neongebung, wir brauchen eine Polizie und wir müssen im Endeffekt dann sozusagen der Rewardfunktion machen. Und für diesen Teil des Linfors mit Learning Rewardfunktion und Umgebung hat man ein dynamisches Sprachmodell benützt, als das man das Sprachmodell benützt, um die Belohnungsfunktion und die Umgebung zu definieren und die auch während dem Training dann durch das Sprachmodell angepasst hat.', 'start': 1011.032, 'end': 1036.783}, {'text': ' Also, wie du gerade beschrieben hast, also den Controller eigentlich, um dynamischer am Reinforcement Learning ganz zu gehen. Und das ganz spannende Wader, das sozusagen so den in dem Drehungsprozess und eine Kognitive Leistung mitgearbeitet ist, damit gelaufen ist. Hat man gar nicht so viel formulieren müssen und der Prozess war effizienter.', 'start': 1036.783, 'end': 1056.852}, {'text': ' ins Gesamt. Und wie im Vosbündung braucht der relativ viel Ressusen, man oder bzw. rechnisch ritte und dann im Endeffekt, so eine Begebnis zu kommen, weil ja man tatsächlich viel ausprobieren muss. Also als Algorithmus. Und das ist natürlich schon interessant, dass man da in so eine generische Richtung gut ist.', 'start': 1056.852, 'end': 1074.684}, {'text': ' Das Ganze dann simulierten Welt macht. Das ist alles virtuell, um die machen, dass sie die Chance haben mit entsprechenden Rechenleistungen der Schnee durchzuführen. Schon bevor du musstest, als Menschen nebenständen im Robert, aber wir dann stürfst die Handdrucken, wenn ich das gleich gelassen hat, wäre ein bisschen blöd. Das Ganze also simuliert, berubertet. Dann kommen aber auch zu einem wichtigen Punkte. Wir haben natürlich diese Kombinationen, Sprachmodelle, Creativität, um die Sachen ausprobieren. Das exakte Durchrechnen,', 'start': 1075.964, 'end': 1101.169}, {'text': ' Und dann aber auch, also wir haben um wie die Sprache mal noch ein bisschen als das, das biniglich, wie sie Sachen, um wie kreativ Sachen erzulge. Ich muss das aber auch nach und nach ergänzen, um um andere Formen der von Sensoren und Wahrnehmung. Also vielleicht das Bild, was ich sehe, dass ich, dass ich mein Umgebung wahrnehmung und einen Umgebungsmodell aufbau.', 'start': 1101.698, 'end': 1120.009}, {'text': ' Und dazu muss ich halt, ja, diese verschiedene Sensorwerte vom Kameras, er hat mir auch schon diskutiert, eine Bild und Sprach zusammenbringen, aber letztendlich unterschiedliche Sensor, die ich habe, Vereinen, eine spannende Frage wird sein, in die Gericht, das alles in einer, in der Hintergrund der Situation, wo ich auch die Sprache drin habe, ist das irgendwie was ergänzendes, für verschiedene Sachen zusammenbringen, dass ich irgendwo dieses, dieses,', 'start': 1120.811, 'end': 1145.111}, {'text': ' Ja, mein Modell, der Umwelt, wie die Welt funktioniert, um mit drin habe. Und da sind wir nämlich auch bei einem großen Krakkomt, meiner Meinung nach der aktuellen Modelle, dass wir diese dieser allgemeinen Wissen, wie funktioniert die Welt kommen, sind es nicht. Das sind der große Schwächen, das wir sowas nach und nach um drei Wochen brauchen können.', 'start': 1146.015, 'end': 1163.08}, {'text': ' Ja, und das ist auch das, was man jetzt sieht, momentan, dass immer mehr Multimodale Modelle auskommen, dass die effizient sind, dass man auch mit GBT-4V und Modelle hat es zwischen den Kennenkern, das Bild der Genetinkern, mit dem Dal-E, alles kombiniert in einem Modell, eigentlich kann man dann das nicht wirklich ein Modell, aber in einer Applikation, so dass ich jetzt anwende, wirklich in eine Inderaktion bin, mit sehen, hören, fühlen vielleicht noch nicht, aber sehen, hören, erstellen, mal und so weiter.', 'start': 1164.241, 'end': 1193.336}, {'text': ' Ich finde aber auch ehrlich gesagt, wenn man das so hören. Und das ist glaube ich, dass wir auch in den nächsten Monaten noch einiges zu tun haben, wenn er als Portcaster', 'start': 1193.336, 'end': 1214.701}, {'text': ' Wenn ich drüber nachdenke, es wird immer komplexer auch von den Modellen her. Also wir gehen ja immer mehr in der Generalisierung rein, wenn nehmen immer mehr, ich glaube, wir sind so ein oder verschiedene Mädchen dazu, also Ton, Bild oder Ändes.', 'start': 1214.701, 'end': 1230.06}, {'text': \" Und es bedeutet aber auch, dass das Modell zwangsläufig immer kreativer wird in der Ausgabe. Sind scheidet ihr dann ob's den Wildmal, ob's den Ton rausgebt und ein Text rausgebt oder ähnliches. Und damit wird es ganz sehr. Und da glaube ich, dass es ein großer Punkt der Moment hat noch vernachlässigt, wird an vielen Ecken. Das ist ganz im Monitoring, Tracking und Evaldeieren. Also wir haben jetzt viel über Evaldeieren schon gemacht, wo ihr seit deinem Bicke mehr Sprachmodelle evaluieren.\", 'start': 1230.06, 'end': 1253.985}, {'text': ' Aber meine Ansicht nach ist genau dieses Tracking und Monitoring. Es ist ganz, ganz wichtiger Part in der Zukunft, wo wir sagen, wie kann ich das überhaupt in dem Produktivunternehmen machen? Wie schaff ich das ein Modell, dass ich vielleicht benütze, mal im Unternehmen über die gesamte Laufzeit zu überhaben.', 'start': 1253.985, 'end': 1274.974}, {'text': ' Was aber ein riesiges, ob noch mal den Bogen mal kurz zurück, wenn wir mit dem überwachen oder bewerten, evaluieren, wie die kreativer die Leistung ist, wie ich gewünsche. Das ist wie schwieriger, es ist ja da vernünftig zu evaluieren. Was ist, das ist schon was richtig im falsches, wenn eine kreative Sache da ist, dann ist es schwierig. Aber wenn man zurück zu diesem Thema kreativität glaubst du denn, dass sich solche Modelle wirklich kreativ sind.', 'start': 1274.974, 'end': 1298.234}, {'text': ' Das ist die Frage, was du unter kreativität verstehst. Also wenn ich sehe, ich habe eine verschiedeneartige Variation von dem Output, wo ich vorher nicht selber draufgekommen werde, würde ich sagen, ja, das nicht gewisse kreativität da. Wenn ich natürlich sagt, das ist was komplett Neues, was noch nie irgendwie existent war. Dann würde ich eher sagen, wahrscheinlich eher nicht.', 'start': 1300.111, 'end': 1321.015}, {'text': ' Man könnte sagen, eigentlich fest die Modelle machen. Das ist ja eigentlich, dass sie nur im Kontext abhängig Sachen fortfühlen in einer Art und Weise, die in dem Kontext um die wahrscheinlichste Variante ist, sofern eigentlich immer nur Sachen, die es oben schon mal gesehen hat, das müsste. Aber tust du das nicht als Künstler?', 'start': 1321.015, 'end': 1337.227}, {'text': ' Das wollte ich nämlich aus sagen. Das spricht ein bisschen gegen so eine echte Kreativität. Aber das mache ich dann als Mensch. Hab ich als Mensch immer wieder kompletten Neuesachen, weil ich denke nicht. Ich glaube schon, dass es vielfach einfach, man hat Associationen und diese Associationen mit Dingen, die man schon mal um mitgekriegt hat.', 'start': 1337.227, 'end': 1352.568}, {'text': ' Diesen einfach da und das man vielleicht übergreifen kann oder formulieren kann. Aber man hat diese und bringt die einfach einen neuen Zusammenhängen und diese neuen Zusammenhängen sind halt immer unser Kontext. Es ist glaube ich, insofern ist es in vielen Fällen, ich sag das, was wir Menschen auch machen oder einen bestimmtes Thema mit mit anderen Stierrichtungen, um viel noch mal ein bisschen darstellen. Und das können man der da unheimlich gut. Ja, ein Thema sagen, mache es verbessern können so gut. Ich habe ein Thema strebt, dass man mit dem Stil in die Art und Weise', 'start': 1353.097, 'end': 1376.288}, {'text': ' Das klappt unheimlich gut insofern. Vielleicht echt neue Sachen sind bei Menschen, der vielleicht auch seht, das wirklich mal jemand eine neue Stierrichtung präges, weil ich mal Sachen anders. Das ist aber vielleicht auch einfach im Teil zufällt, dass man mal Sachen nochmal anders macht. Und dann kommt es gut an. Und dann ist es wieder nett. Also insofern braucht man vielleicht aber hier mit mir ein bisschen zufällt, ein reinbringen und dann ist das wecker nicht so viel.', 'start': 1376.732, 'end': 1398.336}, {'text': ' Die Frage ist jetzt aber, das ist genau das, was ich mein mit dem Variation, weil im Endeffekt du mir im Endeffekt so ein Modell nochmal ausführen oder dann ist das Endeffekt eine Zufahrtskombonein, der mit dabei. Das ist eine Phase zu sein, wir von einer Stadtpunkte aus geht oder mehr andere Sieden, um eine Zufahrtskombonein verwendet wird und dann kommt dann an der Output. Das wird eigentlich ein Zufahrtspunkt. Genau. Und jetzt ist es so, wenn natürlich gibt es Rückblick, kann es natürlich sagen, wo einer Vergangenheit, aber der und der Künstler, der sowas tolles gemacht und das war super kreativ.', 'start': 1398.336, 'end': 1428.336}, {'text': ' Ja, das ist der Teil der Umfohängeblieben ist, wo die Leute gesagt haben, das gefällt und wie viele andere Künstler gehabt ist, die irgendwas gemacht haben, was keiner mochte, was aber auch anders war, was aber irgendwie und natürlich gefallen ist, weil es irgendwie nicht das Interesse gewickt hat. Oder war ihr auch den Wetter von Kreativ? Ja, oder was wir erfahren hatte, die diese Person im Vorfeld, und hat vorher sozusagen, wird beschrittweise sich in dieser Richtung evaluiert, sozusagen, was auch in der Position ist.', 'start': 1428.336, 'end': 1453.114}, {'text': ' Also sofern kann man vielleicht schon sagen, dass eine gewisse Kreativität da ist. Und die Diskussion, was wirklich kreativität ist, kann man nicht lösen. Aber was ich interessant finde, wir haben ja einmal schon über das Thema hallucination gesprochen und kreativität. Man ist nicht das ist eigentlich das Service.', 'start': 1453.114, 'end': 1468.985}, {'text': ' Das weiß ich nicht und möchte ich da auch glaube ich nicht anbauen. Was ich aber ... Wenn man könnte zum Beispiel sagen, als es prinzipisch ist, dass selber nur alle Zinationen ist, ist es, wenn man es nicht wollen, wenn halt einfach faszten, wenn man eigentlich fakten uns wünschen und was anderes kommt und kredit ist, wenn es in der Situation ist, in der wir uns wünschen, dass durchaus was kommt, was nicht so bekannt ist.', 'start': 1469.804, 'end': 1491.203}, {'text': ' Ich finde, er nicht des spannende, weil ehrlich gesagt, es gibt ja so Studien, wo man ausgefunden hat, dass der Mensch sozusagen auch andere Vorstellungen hat, von der er in seinem Kopf, was er erlebt hat und er erinnert hat, als das, was Realität war. Also umso länger, was wegges, umso mehr verwischte Realität, mit dem, was du meinst, das war.', 'start': 1491.374, 'end': 1510.333}, {'text': ' Und das ist ja auch Halluzinien. Und deswegen fand ich jetzt ganz lustiges, glaube ich, ehrlich gesagt habe ich in Titel nicht kauft, weil es aber auch gar nicht so wichtig. Wir jetzt die letzte Tage im Paper von Microsoft ging vor allem halt um moderne Größe und so weiter. Und zwischen drin haben sie so vergleichige Zogen und haben einen Satze und wie ich dann geschrieben, dass bei mir hingeblieben ist. So hat die Motto, es ist auch unglaublich, ob nicht der Mensch eine der besten und effizientesten Next-Wort-Britiction-Maschinen ist.', 'start': 1510.333, 'end': 1537.637}, {'text': ' Also mit das Wissen, ob so viel mehr ist, als das Festival hier mir das auch nur, dass wir mitgefühlen haben und auch ein Bass, was hat das so gelernt haben, weiß man, ja, aber das ist immer ein Mal ich genau bei dem Punkt, was uns ja vielleicht noch unterscheidet von diesem klassischen Ansatz, also die ist diesen Teil der näher natürlich, ich generiere Wort für Wort, Token für Token, dem ich spreche, da kommt sowas raus.', 'start': 1537.637, 'end': 1555.845}, {'text': ' Ja, klar, dass das passiert, aber wir haben ja auch noch eine größere Form von Gefühlen, Emotionen und solche Geschichten, die in den aktuellen Modellen nicht mit drin sind. Ja, absolut. Also, so mal, was ist das so eine bio- chemischen Ebene noch opassiert und in unserem Höhen? Und in eigene Antrieb? Ja. Das ist, hatten wir auch schon mal gesagt, eins der wichtigsten Titel. Wo geht es die Reise hin? Vor allem mit den Sprachen Modellen? Was meinst du?', 'start': 1555.845, 'end': 1579.872}, {'text': ' Ja, also ich glaube, das nach und nach mehr und mehr, also diese Brötigemodalität des unterschiedlichste Arten von Daten, was wir in unserer Regierung aufbühren, mit stärker integriert wird.', 'start': 1580.981, 'end': 1592.5}, {'text': ' und da einfach mehr Fähigkeiten da sind. Ich hoffe ein bisschen, dass man nicht nur so riesengroße Modelle, die alles umfuckern macht, sondern dass diese Zahlregung und Stück weit auch auch zumindest in Lösungen sich durchsetzt, dass ich zwei so eine gewisse Fähigkeit zu schließen zu planen.', 'start': 1593.029, 'end': 1613.746}, {'text': ' Also wir sind immer Schritt für Schritt, dass ich die Sprache als solche als Grundfertigkeit, aber ich brauchen gut gut formulieren zu können. Auch ich habe gar nicht so riesen Modelle, das zeigen ja die Evaluierung von auftem Sort mit den kleineren gut formulieren, kann ich mit einem viel kleineren Modelle.', 'start': 1613.746, 'end': 1629.019}, {'text': ' Und und brauche ich wirklich einen riesen Sprachmodell, wo alles reingestopft ist, an irgendwelchen Wissensfakmenten, die umreproduziert werden sollen. Oder ist es nicht effizienter zu sagen, dass, was ich wirklich als Fakten gesichert habe möchte und ob er abgelegt habe, habe ich in Wissenskrafen, in irgendwelchen anderen Datenbanken. Und und habe einfach nur eine sehr intelligente Form, dass zu kombinieren, mit entsprechenden, also wieder die Exakte, Berechnung von bestimmten Sachen, die kreative Geschichten und das einfach sinnvoll zusammengesponnen,', 'start': 1629.019, 'end': 1656.715}, {'text': \" Und ich hoffe, dass so ein bisschen die Lösung das stärker aufgreifen und auch prüchst dich geben. Ja, das ist ganz interessant, weil Herr mit's gerade auch mit dem aus unser Forschungsrichtung herausgekuckt, das ist für das nächste Jahr für uns interessant. Und da kann man eigentlich genau dieses, was du jetzt so ein bisschen es geziert hast, wo du hast, wo es hingeht, aber auch ganz konkret einig begründet. Einig kann man die Spamende, die wir so haben, in Amerika, die jullie hineinteilen.\", 'start': 1656.715, 'end': 1682.193}, {'text': ' Es gibt, würde ich mal sagen, so die Mega-Elems, das sind so alles, was, da man mal, das sind 100 Milliarden per Meter oder, eigentlich müsste man sagen, 50 Milliarden per Meter größer ist. Das sind alles Modelle, die du nicht in einer einfachen Infrastruktur und mit einfach mal jetzt nicht in der Haus und Roof, private GPU oder irgendwas an der halte, eine einzige GPU auch, um den Teurer betreiben kannst. Also so, dass die Investitionen relativ teuer sind, wenn ich sowas betreiben möchte.', 'start': 1682.193, 'end': 1711.442}, {'text': \" Solche Modelle sind natürlich enorm mächtig, das sieht man, dem gebete Fier. Die können im Endeffekt viele Fiedern, wenn uns gebieder abdecken, die sind da wahnsinnig gut. Aber halt auch sehr kosteninnensiv und deswegen wird's die Himainer an sich nach, eigentlich nur bei den Bektechkompanies in den nächsten Jahren geben. Und alles nur über Applied, also solche Riesenmodelle, wirst du nicht finden, in irgendeiner Unternehmen, das im Endeffekt hat ich betreibt mein kleines.\", 'start': 1711.442, 'end': 1737.432}, {'text': ' Dann meine Ansicht nach ist dann so die nächste Kategorie, die Enterprise Elements. Und das ist so eine Größenordnung zwischen 7 und 40 Milliarden km. Weil das ist genau die Größenordnung, die du in eine einzige große Grafikate, natürlich ist gerade schon eine spezialisierte Hardware reinpasst und die du selber gut trainieren kannst. Und natürlich dann auch auskosten sich gut in der Infrarienz, also in den Betrieb des Models innerhalb deiner Organisation nutzen kannst.', 'start': 1737.432, 'end': 1765.759}, {'text': ' Und das ist ja eigentlich genau die Größenordnung 47 Milliardenparameter, wo eigentlich die ganzen Obensoßmodell darauf sind. Und wo ja wahnsinnig viel bewegt, also über diesen 50 oder 40 Milliardenparameter passiert ja nicht viel, weil das einfach zu teuer ist, das zu betreiben, das ist ein wenig ...', 'start': 1765.759, 'end': 1783.285}, {'text': ' Pläher, die dann die schon wahnsinnige Sprünge machen in der Entwicklung, aber halt auf einer ganz anderen Dimension. Und unterhalb dieser 7 Milliarden hat mir auch schon mal gesagt, haben wir das Problem, dass die Modelle gar nicht so stark ihre Riesening, möglichkeiten haben wir so mitdenken können, wenn das so ein Hennen möchte. Und von daher ist es genau die Spannbreide, die man meine Ansicht nahezu in den nächsten 1, 2 Jahren vor allem in den Unternehmen sehen wird.', 'start': 1783.285, 'end': 1806.254}, {'text': ' Einständige Modelle trinieren, die dann ganz spezifisch auf die Anwendungsweise sind. Und dann im Endeffekt. Und das ist eigentlich das, wir bleiben bei einem Multi-Purpose, also mehr Zweck-Modell, also nicht wie bei der klassischen Maschinenörning oder Deep Learning, dass du das sich optimieren auf einen Problem hin.', 'start': 1806.254, 'end': 1821.869}, {'text': ' sondern du wirst sozusagen die Modelle so runter trimmen, dass du dein Anwendungsfall der vielleicht aus mehreren Aspekten besteht. Soll du zusammenfassung oder Extraction oder ja ich sag mal Problemlösung diese drei Punkte dann vielleicht in eine Modelle hast und auf die Trimst du das dann. Und alles darum ist, dass du die das die das besonders gut können, das generell diese Sparche als Benümmetel haben. Das ist schon das ist, man würde ich kurz ergänzen, ja, was du da anspresst.', 'start': 1821.869, 'end': 1847.807}, {'text': ' ist er letztendlich spiegelt sie wieder, dass das nicht nur die Reine Leistung, was das Ding als Fähigkeiten, was es kann, sondern immer wir stärker jetzt auch gerade, weil sie so groß und teuer sind, die andere Kriterien, wie Geschwindigkeit bei der Verarbeitung kostet, die dann nicht erzeugt werden, bei der Bewertung und bei der Umsetzung viel, viel wichtiger werden.', 'start': 1847.807, 'end': 1865.213}, {'text': \" Und dann, und das finde ich eigentlich, dass man jetzt gerade so ein bisschen in der Forschung zieht, wobei es echt zufrieden ist, ist so zu nennen, ja, Minie LL-Ams. Also Minie Ladschlenkelschmottels. Eigentlich eben, und deshalb sollten wir eigentlich auch nur nur von LL-Ams reden, sprach ich noch. Und ob ich in einem großer kleinen Sinn, ist jetzt immer ein großer Sinn. Ich müsste sich's rauskützen.\", 'start': 1865.555, 'end': 1885.538}, {'text': \" Gibt's sicherweise findet man immer ein oder anderen Päber genau diese Bezeichnung, Mini-LM's. Und dieses ist ja die Bandbreite eine Millionate bis fünf Milliarden. Die sind natürlich wahnsinnig klein. Die kannst du sozusagen auch mit Instructionsdaten trainieren, also zwei Tunien, dass sie in ein ganz kleinen Anwendungsfall wirklich sehr gut sind. Und dann aber auch auf Embeddy weiß es, dass auf eben externe Geräten ohne GPU funktionieren. Also ohne hat wir beschleuniger.\", 'start': 1885.538, 'end': 1912.09}, {'text': ' Und damit dann im Endeffekt natürlich den Anwendungsverkehr für den einzelnen Bereitschritt funktionieren. Also hier ist zum Beispiel und dann noch schnell im Beispiel zu nennen. Sind was man schon gesehen hat, sind Roboter im Endeffekt, die einfach gewisse Funktionität können. Also nehmen sie so einen Stabstoffe Roboter, der einfach durch die Wohnung fährt, der es darauf getrimmt, dass er in unserem Anekt, aber sie können sich nicht mit ihm in der agieren, in der normalen Sprache oder ähnliches. Dann können wir Fehler absetzen.', 'start': 1912.09, 'end': 1935.964}, {'text': ' Und da soll ich mini LM-Strein bauen, die dann bettisch auch in der Aktionen zu lassen, die nicht vorprogrammiert sind. Das ist den Anwennungsfall von diesen ganz kleinen ... Wenn ich den vielleicht aber auch so klein sind, dass hier auch im Betrieb noch um wie leichter angepasst werden können. So ein bisschen, die diese Flexibilität wären der Nutzung, da oben wir noch Sachen verbessern zu können, was bei den Großbritchen der schwierig ist. Ich mache da in diesem Fall, den was du gesteht hat, das immer diese ...', 'start': 1935.964, 'end': 1962.551}, {'text': ' Ich glaube, dass das so ein bisschen Zukunft ist, die Analogie zu uns Menschen wieder zu nehmen. Natürlich können wir bestimmte Probleme mit einem Superexperten oder einer Superexperten, um wie lösen. Aber vielfach ist es so, wir haben ein Team mit Leuten, mit Menschen mit unterschiedlichen Fähigkeiten und die spielen zusammen. Und wenn die Interagieren, kriegen die Umvielösungen hin,', 'start': 1962.551, 'end': 1982.363}, {'text': ' meistens sogar besser sind, als das, was ein super Experten gekriegt hätte, und so ähnlich könnte sich das hier auch entwickeln, dass ich halt einfach verschiedene kleinere Sprachmodelle haben, mit verschiedenen Fähigkeiten und die entsprechende Zusammenbringe und entsprechende Lösung generieren kann, die sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr, sehr,', 'start': 1982.807, 'end': 2000.657}, {'text': ' Ja, das wird definitiv so sein, also von daher neben diesen drei Kategorien wird es dann noch eine oder gibt es schon eine Futterkategorie, also wird es von den Mega zu den Enterprise zu den Mini, noch im Bettingsund Specialist Transformers. Also im Endeffektmodelle, die sich halt ganz stark fukussieren, nur auf die Embeddingerstellung.', 'start': 2001.032, 'end': 2019.48}, {'text': ' Dadurch, dass wir wahnsinnig viel semantische Suge machen können, weil da und wissen, die wir anbitten wollen, an die Spachmodelle, rücken die Embeddings stärker in den Fokus als bisher. Und du kannst ja wahnsinnig wieder rausholen, indem du deinen Embeddingen, dann, also, wie du die Embeddings erzeugst, optimist.', 'start': 2019.48, 'end': 2037.125}, {'text': ' Das ist ganz, ganz wichtig, weil sie bedarsgerecht in unserem und im Kontext im Punkt aufzuwauen müssen wir ja um dieses Wissen oder die Daten, die richtig sind, die ob wir abgelegt sind, bedarsgerecht erst mal auszuwählen und deshalb muss es unheimlich gut klappt.', 'start': 2037.125, 'end': 2053.08}, {'text': ' Und das ist natürlich schon spannend, wie sich der das dann verändert, weil es bedeutet ja im Endeffekt, dass wir eine Vielsteigere und das ist, dass wir immer gesagt haben, mal den Wochen wieder zum Podcast zu schließen, demokratisierung. Also dieses, diese ganz große Fokussieren auf diese Mega-Player auch, dass man auch in Deutschland und auch in anderen Ländern immer hört, wo es die Sovereignität und wir brauchen mindestens so einen ob mehr E-Player in Deutschland, wie auch in USA oder wie auch immer.', 'start': 2053.08, 'end': 2078.848}, {'text': ' Ich glaube, dass das tatsächlich gar nicht so die notwendig sein wird, nur wenn ich kalt sein wird. In den nächsten 1, 2, 3, 4 Jahren, sondern eher die Fokusieren auf diese Breite in der Mitte. Also diese 7 bis 50 Milliarden km. Und ich finde, man hat so eine leichte bestätigt und schon dadurch, weil in einem Päber auch von Microsoft geschrieben, haben man es sich nicht sich auf den Druckfehler ist oder nicht. Haben sie bereits gegeben wie viel Parameter Chat-Shiviti hat. Weißt du wie viel?', 'start': 2078.848, 'end': 2106.476}, {'text': ' Man sitzt wie die 4 oder ... Nee, 3. 5. So, das kleine, so zu sein, das eigentliche Schritt, schiebe ich die. Das sind 20 Milliarden € mit.', 'start': 2106.971, 'end': 2114.923}, {'text': ' Manzt das, was er aus dem ursprünglichen 35 runtergetrieben hat. Man geht davon aus, dass es steht, nicht einfach nur in der Tabelle, weil du Jungs dabei weh haben, sind neue Instruct Chinese gezeigt und hat mal gezeigt, dass das Modell XYP zu lohnt, was hier und auf, ich weiß gar nicht, was das Basis Modell war, besser als als Schedschibitiv und haben in dem Parameter spalten die Parameter mit angegeben. Und eine war halt 20 Milliarden bei Schedschibitiv.', 'start': 2115.179, 'end': 2139.189}, {'text': ' Was ja deutlich weniger ist, als die 175 Milliarden, die wir beim klassischen Budget35 und bei erst mal drin hatten. Genau, und ich meine ansehen nach, ist das Zwangsläufig des Distigieren in dieser Richtung. Und dann habe ich Distigieren optimieren. Und genau dieses Kosteneffizien, weil umso größer die Modelle sind, haben wir das Problem der Infrarienz und damit natürlich das Betriebskosten. Deswegen wird auch auf Dauer gar keinen', 'start': 2139.189, 'end': 2164.138}, {'text': ' 50 Milliarden plus Modell, sich in den Unternehmen rechnen, wenn ich den das Modell in drei oder vier Anwendungsfällen benutze. Oder vielleicht in zehn Anwendungsfällen? Dann drehen ihr, ich liebe sieben zehn Spezialisten. So wie das, wie das er sagt, ja. Und das ist, dass man das stimmt, ein bisschen optimistisch, weil sonst hätten wir die Gefahr, das war eine Handvoll, von, von, von, kleine Handvoll vom Vorm hätten.', 'start': 2164.138, 'end': 2186.391}, {'text': ' die als ich in der Leile sind, später mal die riesengroßen Modelle zu trinieren und zu betreiben. Und wir sind abstuhlt, da habe ich mich gegeben. Mit dem anderen Ansatz haben wir immer noch die Chance, dass man mit einer so mal überschaubaren hat, wird trotzdem noch weitergrennt.', 'start': 2186.391, 'end': 2199.462}, {'text': ' Ja, und das glaube ich einfach recht, dass da nicht sich jetzt in den nächsten 1, 2, 3 Jahren der Markt aufrechen wird. Und zwar massiv. Also nicht nur dieses, wir haben eine Appie und dann dieses Jet-Gibities, sondern es wird tatsächlich auch wieder in die Richtung gehen, dass erst ein startenden ganz große Qualität haben. Wenn wir uns überlegen, die letzten zwei Jahre, man daten ja natürlich notwendig und wir haben alles gesammelt und zu trainieren, verwendet was irgendwie nur irgendwo um lag.', 'start': 2199.514, 'end': 2224.991}, {'text': ' Aber man hat sich nicht den Kopf drüber gemacht. Und zumindest nicht in Gänse zu sagen, die hohe Datenqualität ist es absolut nur ein plus ultra. Was wäre im klassischen Maschinendörning ja schon selangen? Ja. Und das wird sich jetzt auch wieder drehen. Ja? Weil letztendlich beobachtet man, nein, meine, was warum gab es diesen Riesensprungen jetzt in den letzten zwei Jahren? Und warum ist er vielleicht ein bisschen, wenn man von der Geschwindigkeit, wie was es, wie es da weitergeht, glaube ich schon, dass es schon mehr weniger so', 'start': 2224.991, 'end': 2249.991}, {'text': ' Ich habe nicht das Ende erreicht, aber es geht nicht mehr so stark so schnell voran, wie wir es in den vergangenen zwei Jahren hatten. Weil das ist dieser Vergleich zu den ersten Ansatzen vom paar Jahren zu dem, was wir jetzt haben, mal halt, okay, ich habe Zubehufmärle weniger auf alle Daten, die auf dem Internet frei verfügbar sind und wütze sie auch.', 'start': 2250.503, 'end': 2266.715}, {'text': ' Und ich habe die Rechenleistung. Die damit haben wir diesen riesen Sprung geschafft. So jetzt ist ein gewisses Ende erreichen. Natürlich kann ich immer noch mehr Parameter nehmen. Aber irgendwo ist die Menge an Daten, die wir haben, ist mehr eine Wege erschöpft. Erst mal, es er dann werden noch Zugriff auf private Daten von Unternehmen. Aber so mal, dass was frei zugänglich ist, mehr eine weniger Ausgereiz. Jetzt kriegen wir es eher hin, wieder mit dem, dass er sagen, okay, Menge an Daten ist extrem wichtig, für gute Modelle zu bauen, aber noch wichtiger qualitativ hochwertigen Daten.', 'start': 2267.278, 'end': 2294.582}, {'text': ' Das ist der Punkt, dass ich mit weniger hochwertigen Daten und den Sprechenreffenleistung auch sehr gute Vorleihbar gute Modelle in Teilen der Aufgaben bauen.', 'start': 2294.582, 'end': 2302.381}, {'text': ' Ich würde aber nochmal ein Punkt aufgeregen, weil du gesagt hast, die Geschwindigkeit ist nicht mehr so hoch wie in den letzten zwei Jahren. Ich würde das nicht ganz so unterschreiben. Ich würde sagen, die Geschwindigkeit verlagert sich jetzt halt weg von einem Objekt, dass man so zu einer letzten zwei Jahren, er hat im Fokus, hat er nämlich die Gebiete moderne von den Großen anbietern, hin auf eine Breite Masse und die Implementierung und damit auch die Veränderung der Gesellschaft oder der Organisationen in den Unternehmen. Die nimmt meiner uns ihn nach jetzt gerade das Fahrt auf.', 'start': 2303.183, 'end': 2332.363}, {'text': ' Ja, ich wollte damit auch nicht sagen, dass die Entwicklung generell abgebrellen ist, sondern die Entwicklung, was die Anzahl der Parameter in einem Modell angeht. Das ist so ein bisschen zu was erreicht, dass er klar kann, wenn man noch ein bisschen höher gehen. Aber auch auch die BTI-4 hat ja, wenn man das so, wie wir es verstehen, ist ja eher ein Aushaume von mehreren Modell, wo jedes einzelne vielleicht gar nicht so groß ist.', 'start': 2332.483, 'end': 2352.79}, {'text': ' dass man nicht mit dieser Richtung, dass immer mehr Parameter eine Modell drin sein müssen, ist etwas gepremst. Klar, man kann jetzt argumentieren, wenn ich mulche Modelle und viele mehr von meiner Umwelt noch mal annehmen muss, vielleicht müssen die Modelle wieder ein bisschen wachsen, mag der sein. Aber trotzdem ist fast das angeht, die die Entwicklung ist in der andere Richtung gegangen.', 'start': 2352.79, 'end': 2370.691}, {'text': ' Ja, absolut. Und ich finde es auch wieder das, die auch hier wieder parallel. Ja, wenn sieht ja auch nicht unendlich große Unternehmen wachsen. Also irgendwann ist sozusagen ein Unternehmenswachsen, ein Personal, jetzt nicht Umsatz, sondern ein Personal, einfach beschränkt. Ja, und es sind eine Transaktionskosten-Terie und so weiter, die dahinter stecken.', 'start': 2370.811, 'end': 2386.271}, {'text': ' Und das gleiche finde ich hier sieht man genauso. Man kann hier auch von Transaktionskosten sprechen. Und die einfach das Wachstum von unendlich große Modellen einfach nicht zulassen. Weil zwar kann ich damit die Macht zeigen und wie gut was funktioniert. Aber in dem Moment, wo ich es im Betrieb habe, wie jetzt ein paar mal schon erwähnt, dann ist es einfach. Ja, und dann zeigt ich, dass man braucht es. Man braucht es nicht ein Modell, was alles perfekt kann, sondern man braucht ein bestimmten Bereich, halt, denn wieder in sein Spezialgebiete, die, die man gut beherrscht.', 'start': 2386.271, 'end': 2412.483}, {'text': ' Ja. Spannend. Also, Sie sehen unsere Themen gehen nicht aus. Nein. Wir können, glaube ich, noch etwas. Was wir vielleicht tatsächlich mal einer der nächsten Folge aufgreifen können, ist tatsächlich wie das weitergeht, wo das Risiko liegt, da drinnen und ob KI und so einen wirklich wundsorge machen müssen oder wie weit es uns hilft. Ja, der kann ich jetzt schon die Anwürze sagen, ich finde, man muss sich keine Sorgen machen.', 'start': 2412.637, 'end': 2438.012}, {'text': ' Ich denke auch. Das ist einfach das, also ich möchte jetzt nicht mit einem, wenn man uns vielleicht sorgen, in der 100. Episode, nein, das ist eine gute Einwirkung und eine gute Einwirkung zurück, wenn die Frage ist, wie wir es nutzen. Genau. Und das ist glaube ich schon ein wichtiger Vater. Und ich glaube schon, wenn man überlegt, was man da schon für Erleichterungen jetzt auch tut, die Sprachmodelle und auch du Randriker, die Verfahren in den letzten Jahren hatte. Und ich sehe da eigentlich schon auch eine sehr rose Gezugung auf uns zu kommen.', 'start': 2439.633, 'end': 2468.08}, {'text': ' Es hat wahnsinniges Potenzial unsere Fähigkeiten, die wir jetzt schon haben, einfach noch besser zu heben und auszutzen. Und das einzige, was ich halt extrem wichtig finde, und ich denke, dass da brauchen wir auch noch mal ein, zwei, drei, vier, fünf gute Interviewpartner, auch das Thema Winnie-Mädigesellschaft mit.', 'start': 2468.08, 'end': 2484.445}, {'text': ' Das Thema bleibt einfach. Das ist auch, wenn wir am Anfang in den ersten Episode hatten, das ist wichtig, es ist mit Aufklärung. Und so weiter, finde ich, wird es jetzt umso wichtiger, weil die Geschindigkeit, die implementierungen zu nimmt. Und damit die Breite, damit es natürlich auch schwerer ist, gesellschaften, die vielleicht oder Personen, Menschen, die vielleicht nicht die Möglichkeit haben, in allen Bereichen da sich zu informieren, auch mitzunehmen, wenn da sie dadurch genachteil haben.', 'start': 2484.445, 'end': 2509.667}, {'text': ' Ja, da machen wir einen schönes Schlusswort. Genau. Von daher vielen, vielen Dank, wieder fürs Zuhören. Vielen Dank für die letzten 100 Episode, dass ihr unsere Tatkräftig bei Zuhören mitverfolgt haben, gerne auch mit uns Kontakt aufnehmen, wenn sie Themen haben, wo sie gerne einfach sagen. Ach, das sollen die doch mal überreden. Oder wird wir gerne mal von Beitrag mit einspielen. Also gerne sich bei uns melden wir freuen uns. Und wir freuen uns auf die nächsten 100 Episode. Würde ich sagen, oder?', 'start': 2511.681, 'end': 2538.933}, {'text': ' Und die denn hoffentlich nicht wieder ganz lang, wenn wir die heutige Folge aber heute hat man einfach mal viel, viel Rede betroffen. Das trist es. Von daher vielen Dank. Und eine schöne Restwache. Bis dahin, ciao. Das war eine weitere Folge des Knowledge Science Podcasts. Vergesen Sie nicht, nächste Woche wieder dabei zu sein. Vielen Dank fürs Zuhören.', 'start': 2538.933, 'end': 2563.285}], 'language': 'de'}\n",
      "Finished load alignment model...\n",
      "Finished whisperx alignment..\n",
      "Skipping diarization, parsing aligned transcript ...\n",
      "Storing result to ../data/transcripts/16051464880.json ...\n"
     ]
    }
   ],
   "source": [
    "(filename_transcript, transcript) = core.transcribe(episode_id, filename_audio, diarize=False) # , language='de'\n",
    "transcript_parsed = transcript['parsed_transcript']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Create connection...\n",
      "\n",
      "List connections:\n",
      "[('default', <pymilvus.client.grpc_handler.GrpcHandler object at 0x2874c3760>)]\n",
      "exist\n",
      "Connection established\n",
      "exist\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Collection>:\n-------------\n<name>: segment\n<description>: segment\n<schema>: {'auto_id': True, 'description': 'segment', 'fields': [{'name': 'id', 'description': '', 'type': <DataType.INT64: 5>, 'is_primary': True, 'auto_id': True}, {'name': 'embeddings', 'description': '', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 4}}, {'name': 'start', 'description': '', 'type': <DataType.DOUBLE: 11>}, {'name': 'end', 'description': '', 'type': <DataType.DOUBLE: 11>}, {'name': 'speaker', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 10000}}, {'name': 'text', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 10000}}, {'name': 'episode', 'description': '', 'type': <DataType.INT64: 5>}]}"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.main()\n",
    "print('Connection established')\n",
    "db.create_collections()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import db as db\n",
    "import importlib\n",
    "importlib.reload(db)\n",
    "rng = np.random.default_rng(seed=19530)\n",
    "dim=4\n",
    "\n",
    "transcript_parsed = pd.DataFrame([\n",
    "    {'embeddings': rng.random((1, dim))[0], 'speaker': 'unknown', 'start': 0.089, 'end': 1.63, 'text': ' Hallo und herzlich willkommen!', 'episode': 15740694059},\n",
    "    { 'embeddings': rng.random((1, dim))[0],'speaker': 'unknown', 'start': 1.63, 'end': 5.993, 'text': 'In der heutigen Sendung geht es noch einmal um language model programming languages.', 'episode': 15740694059},\n",
    "    { 'embeddings': rng.random((1, dim))[0],'speaker': 'unknown', 'start': 5.993, 'end': 12.497, 'text': 'Diesmal haben wir mit lookerbäurekäne und markfischer 2 Entwickler von LMQL der language model query language zu Gast.', 'episode': 15740694059},\n",
    "    {'embeddings': rng.random((1, dim))[0],'speaker': 'unknown', 'start': 12.497, 'end': 19.361, 'text': 'Die uns spannende Einblicke in die Entstehungsgeschichte und Fähigkeiten von LMQL und mögliche Weiterentwicklung geben werden.', 'episode': 15740694059},\n",
    "    {'embeddings': rng.random((1, dim))[0],'speaker': 'unknown', 'start': 19.361, 'end': 24.404, 'text': 'Auch diese Sendung wird von XZ2 dem Joint Venture von Audi und Cup Gemini gesponsat.', 'episode': 15740694059}\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entities in Milvus: 5\n"
     ]
    }
   ],
   "source": [
    "db.insert(\"segment\", transcript_parsed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'start': 0.089,\n  'end': 1.63,\n  'text': ' Hallo und herzlich willkommen!',\n  'episode': 15740694059,\n  'embeddings': [0.6378742, 0.43925104, 0.13211584, 0.46866667],\n  'id': 445284326180690064,\n  'speaker': 'unknown'},\n {'start': 1.63,\n  'end': 5.993,\n  'text': 'In der heutigen Sendung geht es noch einmal um language model programming languages.',\n  'episode': 15740694059,\n  'embeddings': [0.7442965, 0.03190612, 0.31691247, 0.6025374],\n  'id': 445284326180690065,\n  'speaker': 'unknown'},\n {'start': 5.993,\n  'end': 12.497,\n  'text': 'Diesmal haben wir mit lookerbäurekäne und markfischer 2 Entwickler von LMQL der language model query language zu Gast.',\n  'episode': 15740694059,\n  'embeddings': [0.9007387, 0.44944635, 0.18477614, 0.42930314],\n  'id': 445284326180690066,\n  'speaker': 'unknown'},\n {'start': 12.497,\n  'end': 19.361,\n  'text': 'Die uns spannende Einblicke in die Entstehungsgeschichte und Fähigkeiten von LMQL und mögliche Weiterentwicklung geben werden.',\n  'episode': 15740694059,\n  'embeddings': [0.40345728, 0.3957196, 0.6963897, 0.24356908],\n  'id': 445284326180690067,\n  'speaker': 'unknown'},\n {'start': 19.361,\n  'end': 24.404,\n  'text': 'Auch diese Sendung wird von XZ2 dem Joint Venture von Audi und Cup Gemini gesponsat.',\n  'episode': 15740694059,\n  'embeddings': [0.42512414, 0.5724385, 0.42719918, 0.8820724],\n  'id': 445284326180690068,\n  'speaker': 'unknown'}]"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.get_collection_data('segment')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}